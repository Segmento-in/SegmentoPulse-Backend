# Web Framework
fastapi==0.115.5
uvicorn[standard]==0.32.1
pydantic==2.10.3
python-multipart==0.0.6

# HTTP Client (for model downloads and health checks)
httpx==0.26.0
requests==2.31.0

# Llama-cpp-python - CPU-optimized LLM inference
# Will be compiled with CMAKE_ARGS from Dockerfile
llama-cpp-python==0.2.90

# GLiNER - Fast CPU-based NER
gliner==0.2.19

# Edge-TTS - Cloud-based TTS (zero local resources)
edge-tts==6.1.15

# HuggingFace Hub - Model downloads
huggingface-hub==0.26.5

# Logging and utilities
python-dotenv==1.0.0
