"""
Manual News Fetcher Test
Triggers ingestion and verifies storage and retrieval for cloud category
"""

import asyncio
import sys
import os
import logging
from datetime import datetime

sys.path.append(os.getcwd())

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_full_ingestion_flow():
    """
    Test complete flow:
    1. Manually trigger news fetch
    2. Verify articles are stored in Appwrite
    3. Retrieve cloud category articles
    4. Verify cache population
    """
    print("=" * 80)
    print("üì∞ MANUAL NEWS FETCHER TEST")
    print("=" * 80)
    
    try:
        # Load environment
        from dotenv import load_dotenv
        load_dotenv()
        
        from app.services.scheduler import fetch_all_news
        from app.services.appwrite_db import get_appwrite_db
        from app.services.upstash_cache import get_upstash_cache
        
        # Step 1: Manually trigger ingestion
        print("\nüöÄ Step 1: Triggering manual news fetch...")
        print("-" * 80)
        
        await fetch_all_news()
        
        print("\n‚úÖ Ingestion complete!")
        
        # Step 2: Verify cloud articles in database
        print("\nüîç Step 2: Checking cloud category articles in database...")
        print("-" * 80)
        
        db = get_appwrite_db()
        
        # Test multiple cloud categories
        cloud_categories = [
            "cloud-aws",
            "cloud-azure", 
            "cloud-gcp",
            "cloud-computing"
        ]
        
        for category in cloud_categories:
            articles = await db.get_articles(category, limit=5)
            print(f"   üìä {category}: {len(articles)} articles found")
            
            if articles:
                # Show first article details
                first = articles[0]
                print(f"      Sample: {first.get('title', 'N/A')[:60]}...")
        
        # Step 3: Verify cache population
        print("\nüíæ Step 3: Checking Upstash cache...")
        print("-" * 80)
        
        cache = get_upstash_cache()
        
        if cache.enabled:
            # Check cache for cloud-aws
            cached_data = cache.get("news:cloud-aws")
            if cached_data:
                print(f"   ‚úÖ Cache HIT for cloud-aws: {len(cached_data)} articles")
            else:
                print(f"   ‚ö†Ô∏è  Cache MISS for cloud-aws (normal if first run)")
        else:
            print("   ‚ö†Ô∏è  Cache disabled")
        
        # Step 4: Test API endpoint retrieval
        print("\nüåê Step 4: Testing API endpoint retrieval...")
        print("-" * 80)
        
        # Simulate API call
        test_category = "cloud-aws"
        articles = await db.get_articles(test_category, limit=10)
        
        print(f"   GET /api/news/{test_category}?limit=10")
        print(f"   Response: {len(articles)} articles")
        
        if articles:
            print(f"\n   üìã Sample Articles:")
            for i, article in enumerate(articles[:3], 1):
                print(f"      {i}. {article.get('title', 'N/A')[:70]}")
                print(f"         Source: {article.get('source', 'N/A')}")
                print(f"         Published: {article.get('publishedAt', 'N/A')}")
                print()
        
        # Summary
        print("=" * 80)
        print("üéâ TEST COMPLETE - All systems verified!")
        print("=" * 80)
        print("\n‚úÖ Summary:")
        print("   - Ingestion: Successfully fetched and stored articles")
        print("   - Database: Cloud articles retrievable")
        print("   - Cache: Ready for subsequent requests")
        print("   - API: End-to-end flow working")
        
    except Exception as e:
        print(f"\n‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_full_ingestion_flow())
