"""
Background Scheduler Service - Phase 3
Automates news fetching and database cleanup using APScheduler
"""
import asyncio
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.triggers.cron import CronTrigger
from datetime import datetime, timedelta
import logging
import pytz

from app.services.news_aggregator import NewsAggregator
from app.services.appwrite_db import get_appwrite_db
from app.services.cache_service import CacheService
from app.services.adaptive_scheduler import get_adaptive_scheduler, AdaptiveScheduler
from app.config import settings

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize scheduler
scheduler = AsyncIOScheduler()

# All news categories
CATEGORIES = [
    "ai",
    "data-security",
    "data-governance",
    "data-privacy",
    "data-engineering",
    "data-management",
    "business-intelligence",
    "business-analytics",
    "customer-data-platform",
    "data-centers",
    "cloud-computing",
    "magazines",
    "data-laws",
    # Official Cloud Categories
    "cloud-aws",
    "cloud-azure",
    "cloud-gcp",
    "cloud-oracle",
    "cloud-ibm",
    "cloud-alibaba",
    "cloud-digitalocean",
    "cloud-huawei",
    "cloud-cloudflare"
]


async def fetch_all_news():
    """
    Background Job: Parallel news fetching for all categories (FAANG-Level)
    
    Performance Improvements:
    - Sequential (OLD): 12 categories Ã— 30s each = 6 minutes
    - Parallel (NEW): All 12 at once = 30 seconds = 12x faster!
    
    Runs every 15 minutes to keep database fresh with latest articles.
    """
    start_time = datetime.now()
    
    logger.info("â•" * 80)
    logger.info("ğŸ“° [NEWS FETCHER] Starting PARALLEL news fetch...")
    logger.info("ğŸ• Start Time: %s", start_time.strftime('%Y-%m-%d %H:%M:%S'))
    logger.info("ğŸš€ Mode: Concurrent (asyncio.gather)")
    logger.info("â•" * 80)
    
    # Phase 4: Enhanced tracking for observability
    total_fetched = 0
    total_saved = 0
    total_duplicates = 0
    total_errors = 0
    total_invalid = 0
    total_irrelevant = 0  # NEW: Track category pollution
    category_stats = {}
    
    # FAANG Optimization: Parallel fetch all categories at once!
    fetch_tasks = []
    for category in CATEGORIES:
        task = fetch_and_validate_category(category)
        fetch_tasks.append(task)
    
    # Execute all fetches concurrently with error isolation
    logger.info("âš¡ Launching %d parallel fetch tasks...", len(CATEGORIES))
    results = await asyncio.gather(*fetch_tasks, return_exceptions=True)
    
    # Process results
    appwrite_db = get_appwrite_db()
    cache_service = CacheService()
    
    for result in results:
        # Handle errors gracefully
        if isinstance(result, Exception):
            logger.error("âŒ Fetch task failed: %s", str(result))
            total_errors += 1
            continue
        
        category, articles, invalid_count, irrelevant_count = result
        
        if not articles:
            logger.warning("âš ï¸  No valid articles for category: %s", category)
            category_stats[category] = {
                'fetched': 0,
                'saved': 0,
                'duplicates': 0,
                'invalid': invalid_count,
                'irrelevant': irrelevant_count  # NEW
            }
            continue
        
        try:
            # Save to Appwrite database (L2)
            logger.info("ğŸ’¾ Saving %d articles for %s...", len(articles), category.upper())
            saved_count = await appwrite_db.save_articles(articles)
            
            # Calculate duplicates
            duplicates = len(articles) - saved_count
            
            total_fetched += len(articles)
            total_saved += saved_count
            total_duplicates += duplicates
            total_invalid += invalid_count
            total_irrelevant += irrelevant_count  # NEW
            
            # Store category stats
            category_stats[category] = {
                'fetched': len(articles),
                'saved': saved_count,
                'duplicates': duplicates,
                'invalid': invalid_count,
                'irrelevant': irrelevant_count  # NEW
            }
            
            # Update Redis cache (L1) if available
            try:
                await cache_service.set(f"news:{category}", articles, ttl=settings.CACHE_TTL)
                logger.info("âš¡ Redis cache updated for %s", category)
            except Exception as e:
                logger.debug("âš ï¸  Redis unavailable: %s", e)
            
            logger.info("âœ… %s: %d fetched, %d saved, %d duplicates, %d invalid", 
                       category.upper(), len(articles), saved_count, duplicates, invalid_count)
                       
        except Exception as e:
            total_errors += 1
            category_stats[category] = {'error': str(e), 'invalid': invalid_count}
            logger.error("âŒ Error saving %s: %s", category, str(e))
    
    # Phase 4: Structured end-of-run report
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    logger.info("")
    logger.info("â•" * 80)
    logger.info("ğŸ‰ [NEWS FETCHER] RUN COMPLETED")
    logger.info("â•" * 80)
    logger.info("ğŸ“Š SUMMARY STATISTICS:")
    logger.info("   ğŸ”¹ Total Fetched: %d articles", total_fetched)
    logger.info("   ğŸ”¹ Total Saved (New): %d articles", total_saved)
    logger.info("   ğŸ”¹ Total Duplicates Skipped: %d articles", total_duplicates)
    logger.info("   ğŸ”¹ Total Invalid Rejected: %d articles", total_invalid)
    logger.info("   ğŸ”¹ Total Irrelevant Rejected: %d articles (category pollution)", total_irrelevant)
    logger.info("   ğŸ”¹ Total Errors: %d categories", total_errors)
    logger.info("   ğŸ”¹ Categories Processed: %d/%d", len(CATEGORIES) - total_errors, len(CATEGORIES))
    logger.info("   ğŸ”¹ Deduplication Rate: %.1f%%", (total_duplicates / total_fetched * 100) if total_fetched > 0 else 0)
    total_rejected = total_invalid + total_irrelevant
    logger.info("   ğŸ”¹ Acceptance Rate: %.1f%%", (total_fetched / (total_fetched + total_rejected) * 100) if (total_fetched + total_rejected) > 0 else 0)
    logger.info("")
    logger.info("â±ï¸  PERFORMANCE:")
    logger.info("   ğŸ”¹ Start: %s", start_time.strftime('%H:%M:%S'))
    logger.info("   ğŸ”¹ End: %s", end_time.strftime('%H:%M:%S'))
    logger.info("   ğŸ”¹ Duration: %.2f seconds", duration)
    logger.info("   ğŸ”¹ Throughput: %.1f articles/second", total_fetched / duration if duration > 0 else 0)
    logger.info("   ğŸ”¹ Speed Improvement: ~12x faster than sequential")
    logger.info("â•" * 80)
    
    # FAANG Optimization: Update adaptive scheduler intervals
    from app.services.adaptive_scheduler import get_adaptive_scheduler
    
    adaptive = get_adaptive_scheduler(CATEGORIES)
    if adaptive:
        # Update intervals based on this run's statistics
        for category, stats in category_stats.items():
            if 'fetched' in stats:
                new_interval = adaptive.update_category_velocity(
                    category, 
                    stats['fetched']
                )
        
        # Print adaptive scheduler summary
        adaptive.print_summary()


async def fetch_and_validate_category(category: str) -> tuple:
    """
    Fetch and validate articles for a single category
    
    New: Now includes date normalization and category relevance checks!
    
    Returns: (category, valid_articles, invalid_count, irrelevant_count)
    """
    from app.utils.data_validation import is_valid_article, sanitize_article, is_relevant_to_category
    from app.utils.date_parser import normalize_article_date
    
    try:
        logger.info("ğŸ“Œ Fetching %s...", category.upper())
        
        # Fetch from external APIs
        news_aggregator = NewsAggregator()
        
        # FAANG Optimization: Concurrent fetch from Main Provider Chain + Medium + Official Cloud
        # This ensures we get high-quality API news AND Medium blogs AND Official source simultaneously
        main_task = news_aggregator.fetch_by_category(category)
        medium_task = news_aggregator.fetch_from_provider('medium', category)
        official_task = news_aggregator.fetch_from_provider('official_cloud', category)
        
        results = await asyncio.gather(main_task, medium_task, official_task, return_exceptions=True)
        
        # Combine results
        raw_articles = []
        
        # Result 0: Main Provider Chain
        if isinstance(results[0], list):
            raw_articles.extend(results[0])
        
        # Result 1: Medium RSS
        if isinstance(results[1], list):
            if results[1]: # Only log if we found Medium articles
                logger.info("   + Found %d Medium articles for %s", len(results[1]), category)
            raw_articles.extend(results[1])

        # Result 2: Official Cloud
        if isinstance(results[2], list):
            if results[2]:
                logger.info("   + Found %d Official Cloud articles for %s", len(results[2]), category)
            raw_articles.extend(results[2])
        
        if not raw_articles:
            return (category, [], 0, 0)
        
        # Validate, filter, and sanitize
        valid_articles = []
        invalid_count = 0
        irrelevant_count = 0
        
        for article in raw_articles:
            # Step 1: Basic validation (existing)
            if not is_valid_article(article):
                invalid_count += 1
                continue
            
            # Step 2: Category relevance check (NEW!)
            if not is_relevant_to_category(article, category):
                irrelevant_count += 1
                continue
            
            # Step 3: Normalize date to UTC ISO-8601 (NEW!)
            article = normalize_article_date(article)
            
            # Step 4: Sanitize and clean (existing)
            clean_article = sanitize_article(article)
            valid_articles.append(clean_article)
        
        logger.info("âœ“ %s: %d valid, %d invalid, %d irrelevant", 
                    category.upper(), len(valid_articles), invalid_count, irrelevant_count)
        return (category, valid_articles, invalid_count, irrelevant_count)
        
    except asyncio.TimeoutError:
        logger.error("â±ï¸  Timeout fetching %s (>30s)", category)
        return (category, [], 0, 0)
    except Exception as e:
        logger.exception("âŒ Error fetching %s", category)
        return (category, [], 0, 0)


async def cleanup_old_news():
    """
    Background Job: Delete articles older than 48 hours (Data Retention Policy)
    
    Runs daily at midnight to keep Appwrite database within free tier limits.
    Only keeps the last 2 days of articles.
    """
    logger.info("")
    logger.info("â•" * 80)
    logger.info("ğŸ§¹ [CLEANUP JANITOR] Starting cleanup of old news articles...")
    logger.info("ğŸ• Cleanup Time: %s", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    logger.info("â•" * 80)
    
    appwrite_db = get_appwrite_db()
    
    if not appwrite_db.initialized:
        logger.error("âŒ CRITICAL: Appwrite database not initialized!")
        logger.error("âš ï¸  Cleanup cannot proceed - database connection required")
        logger.error("ğŸ’¡ Check Appwrite credentials in environment variables")
        return
    
    try:
        # Calculate cutoff date (48 hours ago)
        retention_hours = 48
        cutoff_date = datetime.now() - timedelta(hours=retention_hours)
        cutoff_iso = cutoff_date.isoformat()
        
        logger.info("ğŸ“‹ Retention Policy: %d hours", retention_hours)
        logger.info("ğŸ“… Cutoff Date: %s", cutoff_date.strftime('%Y-%m-%d %H:%M:%S'))
        logger.info("ğŸ—‘ï¸  Articles published before this will be deleted...")
        
        # Query and delete old articles
        logger.info("ğŸ” Querying Appwrite for old articles...")
        from appwrite.query import Query
        
        response = appwrite_db.databases.list_documents(
            database_id=settings.APPWRITE_DATABASE_ID,
            collection_id=settings.APPWRITE_COLLECTION_ID,
            queries=[
                Query.less_than('published_at', cutoff_iso),
                Query.limit(500)  # Increased from 100 to 500 for better cleanup throughput
            ]
        )
        
        logger.info("ğŸ“Š Found %d old articles to delete", len(response['documents']))
        
        deleted_count = 0
        if len(response['documents']) > 0:
            logger.info("ğŸ—‘ï¸  Deleting articles...")
        
        for doc in response['documents']:
            try:
                appwrite_db.databases.delete_document(
                    database_id=settings.APPWRITE_DATABASE_ID,
                    collection_id=settings.APPWRITE_COLLECTION_ID,
                    document_id=doc['$id']
                )
                deleted_count += 1
                if deleted_count % 10 == 0:
                    logger.info("   Progress: %d articles deleted...", deleted_count)
            except Exception as e:
                logger.error("âŒ Error deleting document %s: %s", doc['$id'], e)
        
        # Clear Redis cache to force refresh from updated database
        logger.info("ğŸ”„ Clearing Redis cache...")
        cache_service = CacheService()
        cache_cleared = 0
        for category in CATEGORIES:
            try:
                await cache_service.delete(f"news:{category}")
                cache_cleared += 1
            except Exception as e:
                logger.debug("âš ï¸  Cache clear skipped for %s: %s", category, e)
        
        if cache_cleared > 0:
            logger.info("âœ… Cache cleared for %d categories", cache_cleared)
        
        logger.info("")
        logger.info("â•" * 80)
        logger.info("ğŸ‰ [CLEANUP JANITOR] COMPLETED!")
        logger.info("ğŸ—‘ï¸  Total Deleted: %d articles", deleted_count)
        logger.info("â° Retention: Articles older than %d hours removed", retention_hours)
        logger.info("ğŸ• Completion Time: %s", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        logger.info("â•" * 80)
        
        # If there are more old articles, schedule another cleanup soon
        if len(response['documents']) >= 100:
            logger.warning("âš ï¸  WARNING: More old articles detected (100+ limit reached)")
            logger.warning("ğŸ“… Additional cleanup will run in next scheduled job")
        
    except Exception as e:
        logger.error("")
        logger.error("â•" * 80)
        logger.error("âŒ [CLEANUP JANITOR] FAILED!")
        logger.error("Error: %s", str(e))
        logger.error("â•" * 80)
        logger.exception("Full traceback:")


def start_scheduler():
    """
    Initialize and start the background scheduler with all jobs
    """
    logger.info("")
    logger.info("â•" * 80)
    logger.info("â° [SCHEDULER] Initializing background scheduler...")
    logger.info("â•" * 80)
    
    # Job 1: Fetch news every 15 minutes
    scheduler.add_job(
        fetch_all_news,
        trigger=IntervalTrigger(minutes=15),
        id='fetch_all_news',
        name='News Fetcher (every 15 min)',
        replace_existing=True
    )
    logger.info("âœ… Job #1 Registered: ğŸ“° News Fetcher")
    logger.info("   â±ï¸  Schedule: Every 15 minutes")
    logger.info("   ğŸ“‹ Task: Fetch news from all providers and update database")
    
    # Job 2: Cleanup old news every 2 hours
    scheduler.add_job(
        cleanup_old_news,
        trigger=IntervalTrigger(hours=2),  # Every 2 hours
        id='cleanup_old_news',
        name='Database Janitor (every 2 hours)',
        replace_existing=True
    )
    logger.info("")
    logger.info("âœ… Job #2 Registered: ğŸ§¹ Database Janitor")
    logger.info("   â±ï¸  Schedule: Every 2 hours")
    logger.info("   ğŸ“‹ Task: Delete articles older than 48 hours (up to 500 per run)")
    logger.info("   ğŸ”¢ Total cleanup capacity: 6,000 articles/day (12 runs Ã— 500)")
    
    # Import newsletter service (lazy import to avoid circular dependencies)
    from app.services.newsletter_service import send_scheduled_newsletter
    
    # IST timezone for newsletter scheduling
    IST = pytz.timezone('Asia/Kolkata')
    
    # Job 3: Morning Newsletter - 7:00 AM IST, Monday-Saturday
    scheduler.add_job(
        send_scheduled_newsletter,
        trigger=CronTrigger(
            hour=7, minute=0,
            day_of_week='mon-sat',
            timezone=IST
        ),
        args=["Morning"],
        id='newsletter_morning',
        name='Morning Newsletter (7 AM IST)',
        replace_existing=True,
        max_instances=1
    )
    logger.info("")
    logger.info("âœ… Job #3 Registered: â˜€ï¸ Morning Newsletter")
    logger.info("   â±ï¸  Schedule: 7:00 AM IST, Monday-Saturday")
    logger.info("   ğŸ“‹ Task: Send curated news to Morning preference subscribers")
    
    # Job 4: Afternoon Newsletter - 2:00 PM IST, Monday-Friday
    scheduler.add_job(
        send_scheduled_newsletter,
        trigger=CronTrigger(
            hour=14, minute=0,
            day_of_week='mon-fri',
            timezone=IST
        ),
        args=["Afternoon"],
        id='newsletter_afternoon',
        name='Afternoon Newsletter (2 PM IST)',
        replace_existing=True,
        max_instances=1
    )
    logger.info("")
    logger.info("âœ… Job #4 Registered: ğŸ“° Afternoon Newsletter")
    logger.info("   â±ï¸  Schedule: 2:00 PM IST, Monday-Friday")
    logger.info("   ğŸ“‹ Task: Send midday update to Afternoon preference subscribers")
    
    # Job 5: Evening Newsletter - 7:00 PM IST, Daily
    scheduler.add_job(
        send_scheduled_newsletter,
        trigger=CronTrigger(
            hour=19, minute=0,
            timezone=IST
        ),
        args=["Evening"],
        id='newsletter_evening',
        name='Evening Newsletter (7 PM IST)',
        replace_existing=True,
        max_instances=1
    )
    logger.info("")
    logger.info("âœ… Job #5 Registered: ğŸŒ™ Evening Newsletter")
    logger.info("   â±ï¸  Schedule: 7:00 PM IST, Daily")
    logger.info("   ğŸ“‹ Task: Send daily digest to Evening preference subscribers")
    
    # Job 6: Weekly Newsletter - Sunday 9:00 AM IST
    scheduler.add_job(
        send_scheduled_newsletter,
        trigger=CronTrigger(
            hour=9, minute=0,
            day_of_week='sun',
            timezone=IST
        ),
        args=["Weekly"],
        id='newsletter_weekly',
        name='Weekly Newsletter (Sunday 9 AM IST)',
        replace_existing=True,
        max_instances=1
    )
    logger.info("")
    logger.info("âœ… Job #6 Registered: ğŸ“… Weekly Newsletter")
    logger.info("   â±ï¸  Schedule: Sunday 9:00 AM IST")
    logger.info("   ğŸ“‹ Task: Send weekly roundup to Weekly preference subscribers")
    
    # Job 7: Monthly Newsletter - 1st of month, 9:00 AM IST
    scheduler.add_job(
        send_scheduled_newsletter,
        trigger=CronTrigger(
            hour=9, minute=0,
            day=1,
            timezone=IST
        ),
        args=["Monthly"],
        id='newsletter_monthly',
        name='Monthly Newsletter (1st, 9 AM IST)',
        replace_existing=True,
        max_instances=1
    )
    logger.info("")
    logger.info("âœ… Job #7 Registered: ğŸ“Š Monthly Newsletter")
    logger.info("   â±ï¸  Schedule: 1st of month, 9:00 AM IST")
    logger.info("   ğŸ“‹ Task: Send monthly intelligence to Monthly preference subscribers")
    
    # Start the scheduler
    logger.info("")
    logger.info("ğŸš€ Starting scheduler engine...")
    scheduler.start()
    logger.info("")
    logger.info("â•" * 80)
    logger.info("âœ… [SCHEDULER] Background scheduler started successfully!")
    logger.info("ğŸ”„ All jobs are now active and running")
    logger.info("â•" * 80)
    logger.info("")


def shutdown_scheduler():
    """
    Gracefully shutdown the scheduler
    """
    logger.info("")
    logger.info("â•" * 80)
    logger.info("â¹ï¸  [SCHEDULER] Shutting down background scheduler...")
    logger.info("â³ Waiting for running jobs to complete...")
    scheduler.shutdown(wait=True)
    logger.info("âœ… [SCHEDULER] Background scheduler shut down successfully")
    logger.info("â•" * 80)
    logger.info("")


# Manual job triggers for testing (can be called from admin endpoints)
async def trigger_fetch_now():
    """Manually trigger news fetch (for testing)"""
    logger.info("")
    logger.info("â•" * 80)
    logger.info("ğŸ”§ [MANUAL TRIGGER] Running fetch job NOW...")
    logger.info("â•" * 80)
    await fetch_all_news()


async def trigger_cleanup_now():
    """Manually trigger cleanup (for testing)"""
    logger.info("")
    logger.info("=" * 80)
    logger.info("ğŸ”§ [MANUAL TRIGGER] Running cleanup job NOW...")
    logger.info("=" * 80)
    await cleanup_old_news()


async def trigger_newsletter_now(preference: str):
    """Manually trigger newsletter for specific preference (for testing)"""
    from app.services.newsletter_service import send_scheduled_newsletter
    
    logger.info("")
    logger.info("=" * 80)
    logger.info(f"ğŸ”§ [MANUAL TRIGGER] Running {preference} newsletter job NOW...")
    logger.info("=" * 80)
    result = await send_scheduled_newsletter(preference)
    return result
