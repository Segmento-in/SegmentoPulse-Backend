"""
URL Deduplication Service using Scalable Bloom Filter
======================================================

Provides persistent, auto-scaling URL deduplication to prevent
processing the same article multiple times across fetcher runs.

Features:
- **Scalable Bloom Filter**: Automatically grows as more URLs are added
- File-backed persistence (survives server restarts)
- Extremely low memory footprint (~20-50MB for 10M URLs)
- 0.1% false-positive rate (configurable)
- Thread-safe operations
- Statistics tracking

Key Improvements:
- No more saturation issues - filter grows automatically
- Handles infinite news streams
- Optimized for 16GB RAM environment (HuggingFace Spaces)
"""

import os
import pickle
from datetime import datetime
from typing import Optional
import logging
from pybloom_live import ScalableBloomFilter

logger = logging.getLogger(__name__)


class URLFilter:
    """
    Scalable Bloom Filter-based URL deduplication service
    
    Uses a probabilistic data structure that automatically scales
    to efficiently track unlimited URLs with minimal memory overhead.
    """
    
    def __init__(
        self, 
        initial_capacity: int = 10000, 
        error_rate: float = 0.001,
        persistence_path: str = "data/bloom_filter.bin",
        mode: int = ScalableBloomFilter.SMALL_SET_GROWTH
    ):
        """
        Initialize Scalable URL filter
        
        Args:
            initial_capacity: Starting capacity (default: 10K URLs)
            error_rate: Acceptable false-positive rate (default: 0.1%)
            persistence_path: Path to save/load filter state
            mode: Growth mode for scalable filter
                  - SMALL_SET_GROWTH: Doubles capacity each time (2x, 4x, 8x...)
                  - LARGE_SET_GROWTH: Grows by 4x each time (faster but more memory)
        """
        self.initial_capacity = initial_capacity
        self.error_rate = error_rate
        self.persistence_path = persistence_path
        self.mode = mode
        
        # Statistics tracking
        self.stats = {
            'total_checks': 0,
            'duplicates_detected': 0,
            'unique_urls_added': 0,
            'filter_buckets': 0,  # Number of internal filters created
            'last_reset': datetime.now().isoformat(),
            'last_save': None
        }
        
        # Initialize or load scalable bloom filter
        self.bloom_filter = self._load_or_create_filter()
        
        # Update bucket count
        self.stats['filter_buckets'] = len(self.bloom_filter.filters) if hasattr(self.bloom_filter, 'filters') else 1
        
        logger.info("â•" * 70)
        logger.info("ðŸŽ¯ [URL FILTER] Scalable Bloom Filter initialized")
        logger.info(f"   Initial Capacity: {initial_capacity:,} URLs")
        logger.info(f"   Error Rate: {error_rate * 100}%")
        logger.info(f"   Growth Mode: {'SMALL_SET' if mode == ScalableBloomFilter.SMALL_SET_GROWTH else 'LARGE_SET'}")
        logger.info(f"   Active Buckets: {self.stats['filter_buckets']}")
        logger.info(f"   Persistence: {persistence_path}")
        logger.info(f"   ðŸ’¡ This filter can handle UNLIMITED URLs (auto-scales)")
        logger.info("â•" * 70)
    
    def _load_or_create_filter(self) -> ScalableBloomFilter:
        """Load existing filter from disk or create a new one"""
        # Ensure data directory exists
        os.makedirs(os.path.dirname(self.persistence_path), exist_ok=True)
        
        # Try to load existing filter
        if os.path.exists(self.persistence_path):
            try:
                with open(self.persistence_path, 'rb') as f:
                    bloom_filter = pickle.load(f)
                
                # Verify it's a ScalableBloomFilter
                if not isinstance(bloom_filter, ScalableBloomFilter):
                    logger.warning(f"âš ï¸  Loaded filter is not Scalable. Upgrading...")
                    return self._create_new_filter()
                
                logger.info(f"âœ… Loaded existing Scalable Bloom Filter from {self.persistence_path}")
                logger.info(f"   ðŸ“Š Filter has {len(bloom_filter.filters)} bucket(s)")
                return bloom_filter
            except Exception as e:
                logger.warning(f"âš ï¸  Failed to load filter: {e}. Creating new one.")
        
        # Create new filter
        return self._create_new_filter()
    
    def _create_new_filter(self) -> ScalableBloomFilter:
        """Create a new scalable bloom filter"""
        logger.info(f"ðŸ†• Creating new Scalable Bloom Filter")
        logger.info(f"   Starting size: {self.initial_capacity:,} URLs")
        logger.info(f"   Will auto-grow as needed (no size limit!)")
        
        return ScalableBloomFilter(
            initial_capacity=self.initial_capacity,
            error_rate=self.error_rate,
            mode=self.mode
        )
    
    def check_and_add(self, url: str) -> bool:
        """
        Check if URL is new and add it to the filter
        
        Args:
            url: URL to check
            
        Returns:
            True if URL is NEW (not seen before)
            False if URL is DUPLICATE (already processed)
        """
        self.stats['total_checks'] += 1
        
        # Normalize URL (remove trailing slashes, lowercase)
        normalized_url = url.strip().rstrip('/').lower()
        
        # Check if URL exists in the filter
        if normalized_url in self.bloom_filter:
            # URL already exists (duplicate)
            self.stats['duplicates_detected'] += 1
            return False
        
        # URL is new, add it to the filter
        self.bloom_filter.add(normalized_url)
        self.stats['unique_urls_added'] += 1
        
        # Track bucket growth
        current_buckets = len(self.bloom_filter.filters) if hasattr(self.bloom_filter, 'filters') else 1
        if current_buckets > self.stats['filter_buckets']:
            logger.info(f"ðŸ“ˆ [BLOOM FILTER] Auto-scaled! New bucket #{current_buckets} created")
            logger.info(f"   Total capacity now: {self.initial_capacity * (2 ** (current_buckets - 1)):,} URLs")
            self.stats['filter_buckets'] = current_buckets
        
        # Periodically save state (every 100 new URLs)
        if self.stats['unique_urls_added'] % 100 == 0:
            self.save_state()
        
        return True
    
    def save_state(self):
        """Persist Scalable Bloom Filter to disk using pickle"""
        try:
            # Ensure directory exists
            os.makedirs(os.path.dirname(self.persistence_path), exist_ok=True)
            
            # Save bloom filter using pickle (ScalableBloomFilter doesn't have tofile)
            with open(self.persistence_path, 'wb') as f:
                pickle.dump(self.bloom_filter, f)
            
            self.stats['last_save'] = datetime.now().isoformat()
            logger.debug(f"ðŸ’¾ Scalable Bloom Filter saved ({self.stats['filter_buckets']} buckets)")
        except Exception as e:
            logger.error(f"âŒ Failed to save Bloom Filter: {e}")
    
    def get_stats(self) -> dict:
        """Get deduplication statistics"""
        duplicate_rate = (
            self.stats['duplicates_detected'] / self.stats['total_checks'] * 100
            if self.stats['total_checks'] > 0 else 0
        )
        
        # Estimate current capacity (doubles with each bucket)
        estimated_capacity = self.initial_capacity * (2 ** (self.stats['filter_buckets'] - 1))
        
        return {
            **self.stats,
            'duplicate_rate_percent': round(duplicate_rate, 2),
            'initial_capacity': self.initial_capacity,
            'estimated_current_capacity': estimated_capacity,
            'filter_error_rate': self.error_rate,
            'is_scalable': True
        }
    
    def print_stats(self):
        """Print deduplication statistics"""
        stats = self.get_stats()
        
        logger.info("")
        logger.info("â•" * 70)
        logger.info("ðŸ“Š [URL FILTER] Deduplication Statistics (Scalable)")
        logger.info("â•" * 70)
        logger.info(f"   ðŸ”¹ Total Checks: {stats['total_checks']:,}")
        logger.info(f"   ðŸ”¹ Unique URLs Added: {stats['unique_urls_added']:,}")
        logger.info(f"   ðŸ”¹ Duplicates Detected: {stats['duplicates_detected']:,}")
        logger.info(f"   ðŸ”¹ Duplicate Rate: {stats['duplicate_rate_percent']}%")
        logger.info(f"   ðŸ”¹ Active Buckets: {stats['filter_buckets']}")
        logger.info(f"   ðŸ”¹ Current Capacity: {stats['estimated_current_capacity']:,} URLs")
        logger.info(f"   ðŸ”¹ Error Rate: {stats['filter_error_rate'] * 100}%")
        if stats['last_save']:
            logger.info(f"   ðŸ”¹ Last Saved: {stats['last_save']}")
        logger.info("â•" * 70)
        logger.info("")
    
    def reset(self):
        """Reset the filter (use with caution)"""
        logger.warning("âš ï¸  Resetting Scalable Bloom Filter - all history will be lost!")
        self.bloom_filter = ScalableBloomFilter(
            initial_capacity=self.initial_capacity,
            error_rate=self.error_rate,
            mode=self.mode
        )
        self.stats = {
            'total_checks': 0,
            'duplicates_detected': 0,
            'unique_urls_added': 0,
            'filter_buckets': 1,
            'last_reset': datetime.now().isoformat(),
            'last_save': None
        }
        self.save_state()
        logger.info("âœ… Scalable Bloom Filter reset complete")
    
    def get_estimated_memory_usage(self) -> str:
        """
        Estimate current memory usage
        
        Note: ScalableBloomFilter memory grows gradually:
        - 10K URLs â‰ˆ 15 KB
        - 100K URLs â‰ˆ 150 KB  
        - 1M URLs â‰ˆ 1.5 MB
        - 10M URLs â‰ˆ 20-50 MB
        
        Even with 10 million URLs, this uses <50MB of your 16GB RAM!
        """
        estimated_urls = self.stats['unique_urls_added']
        
        # Rough estimate: ~1.5KB per 1000 URLs
        estimated_kb = (estimated_urls / 1000) * 1.5
        
        if estimated_kb < 1024:
            return f"~{estimated_kb:.1f} KB"
        else:
            return f"~{estimated_kb / 1024:.1f} MB"


# Global singleton instance
_url_filter: Optional[URLFilter] = None


def get_url_filter() -> URLFilter:
    """
    Get or create global URL filter instance
    
    Returns:
        URLFilter: Singleton URL filter instance (Scalable)
    """
    global _url_filter
    
    if _url_filter is None:
        _url_filter = URLFilter()
    
    return _url_filter
